<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page for DiscoHuman">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <meta name="robots" content="noindex, nofollow">
  <meta name="keywords" content="DiscoHuman Project, Human Synthesis, Diffusion, Disentanglement">
  <title>DiscoHuman Project Page</title>
  <link rel="icon" type="image/x-icon" href="/assets/ZhengwentaiSUN.webp">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DB33G935YG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DB33G935YG');
  </script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop" style="padding-left: 0; padding-right: 0;">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Exploring Disentangled and Controllable Human Image Synthesis: <br> From End-to-End to Stage-by-Stage</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://taited.github.io/" target="_blank">Zhengwentai Sun</a>,</span>
                <span class="author-block">
                  <a href="https://lhyfst.github.io/" target="_blank">Heyuan Li</a>,</span>
                  <span class="author-block">
                    <a href="https://github.com/TIM2015YXH" target="_blank">Xihe Yang</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/KeruZheng" target="_blank">Keru Zheng</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://ningshuliang.github.io/" target="_blank">Shuliang Ning</a>,
                  </span>
                  <br>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=-LATi7MAAAAJ" target="_blank">Yihao Zhi</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=tcRMmRkAAAAJ" target="_blank">Hongjie Liao</a>,
                  </span>
                   <span class="author-block">
                    <a href="https://kevinlee09.github.io/" target="_blank">Chenghong Li</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://sse.cuhk.edu.cn/en/faculty/cuishuguang" target="_blank">Shuguang Cui</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://gaplab.cuhk.edu.cn/" target="_blank">Xiaoguang Han</a> <sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Chinese University of Hong Kong, Shenzhen</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/Supplementary.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/taited/DiscoHuman" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/discohuman.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
  </section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p class="is-6">
            Achieving fine-grained controllability in human image synthesis is a
            long-standing challenge in computer vision. Existing methods primarily
            focus on either facial synthesis or near-frontal body generation, with
            limited ability to simultaneously control key factors such as viewpoint,
            pose, clothing, and identity in a disentangled manner. In this paper, we
            introduce a new disentangled and controllable human synthesis task, which
            explicitly separates and manipulates these four factors within a unified
            framework. We first develop an end-to-end generative model trained on
            MVHumanNet for factor disentanglement. However, the domain gap between
            MVHumanNet and in-the-wild data produce unsatisfacotry results, motivating
            the exploration of virtual try-on (VTON) dataset as a potential solution.
            Through experiments, we observe that simply incorporating the VTON dataset
            as additional data to train the end-to-end model degrades performance,
            primarily due to the inconsistency in data forms between the two datasets,
            which disrupts the disentanglement process. To better leverage both datasets,
            we propose a stage-by-stage framework that decomposes human image generation
            into three sequential steps: clothed A-pose generation, back-view synthesis,
            and pose and view control. This structured pipeline enables better dataset
            utilization at different stages, significantly improving controllability and
            generalization, especially for in-the-wild scenarios. Extensive experiments
            demonstrate that our stage-by-stage approach outperforms end-to-end models
            in both visual fidelity and disentanglement quality, offering a scalable
            solution for real-world tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <section class="hero teaser">
    <h2 class="title is-4 has-text-centered" style="margin-top: 20px">Disentanglement and control of faces, clothes, shoes, views, and poses.</h2>
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/fig1_teaser_v3.png">
        <p class="subtitle is-6">
          We introduce a new task that explicitly disentangles key human attributes within a unified
          framework. This enables fine-grained and controllable human synthesis.
        </p>
      </div>
    </div>

    <h2 class="title is-4 has-text-centered" style="margin-top: 20px">More challenging poses.</h2>
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/supplementary_more_results_poses.png">
      </div>
    </div>
  </section>

<section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container  has-text-centered">
        <h2 class="title is-3">Method</h2>
      </div>
      <div style="text-align: center; margin-top: 20pt; margin-bottom: 10pt">
        <img src="static/images/fig2-stage-by-stage_250308.png" style="display: block;
          margin-left: auto;
          margin-right: auto; width: 70%;">
      </div>
      <p class="subtitle is-6" style="width: 80%; margin-left: 10%">
        Overview of the proposed pipelines. (a) The end-to-end pipeline directly
        synthesizes the final image from disentangled inputs, including a face
        image, clothing images, and a pose map. (b) The stage-by-stage pipeline
        decomposes the process into three steps: front-view synthesis with identity
        and clothing control, back-view synthesis, and free-view synthesis under
        the target pose and viewpoint. Both pipelines are implemented using DiscoHuman,
        with details provided in below.
      </p>

      <div style="text-align: center; margin-top: 20pt; margin-bottom: 10pt">
        <img src="static/images/fig3%20network_250308_v1.png" style="display: block;
          margin-left: auto;
          margin-right: auto; width: 70%;">
      </div>
      <p class="subtitle is-6" style="width: 80%; margin-left: 10%">
        DiscoHuman model \( \varepsilon \) consists of a VisualDiT \( \varepsilon_V \) and a HumanDiT
        \( \varepsilon_H \). The VisualDiT is responsible for encoding visual conditions,
        with different input settings depending on the pipeline or stage in which DiscoHuman
        is applied. The upper left blocks illustrate three possible input configurations.
        In this figure, the active configuration corresponds to Stage 3, while the inactive
        settings are indicated by grey dashed lines. To maintain simplicity, the denoising
        timestep t is not shown in this figure.
      </p>

    </div>
  </section>

  <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container  has-text-centered">
        <h2 class="title is-3">Comparison</h2>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/mvhuman_comp_1.png" style="max-width: 50%; display: block;
          margin-left: auto;
          margin-right: auto;" alt="MY ALT TEXT"/>
          <p class="subtitle is-6 has-text-centered">
            Comparison on MVHumanNet (1).
          </p>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/mvhuman_comp_2.png" style="max-width: 50%; display: block;
        margin-left: auto;
        margin-right: auto;" alt="MY ALT TEXT"/>
        <p class="subtitle is-6 has-text-centered">
          Comparison on MVHumanNet (2).
        </p>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/avatarrex_comp.png" style="max-width: 50%; display: block;
          margin-left: auto;
          margin-right: auto;" alt="MY ALT TEXT"/>
          <p class="subtitle is-6 has-text-centered">
            Comparison on THuman 4.0 and AvatarRex dataset.
          </p>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/in_the_wild_2.png" style="max-width: 50%; display: block;
          margin-left: auto;
          margin-right: auto;" alt="MY ALT TEXT"/>
          <p class="subtitle is-6 has-text-centered">
            Comparison on in-the-wild data generated by Stable Diffusion.
         </p>
       </div>
      </div>

      <p class="subtitle is-6" style="margin-left: 5%">
        We compare our method against
        <a href="https://github.com/MooreThreads/Moore-AnimateAnyone" target="_blank">AA (AnimateAnyone)</a>,
        <a href="https://github.com/magic-research/magic-animate" target="_blank">MA (MagicAnimate)</a>,
        and
        <a href="https://github.com/YanzuoLu/CFLD" target="_blank">CFLD</a>.
        Our method achieves the best performance across multiple datasets.
      </p>
    </div>
</div>
</section>
<!-- End image carousel -->

<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Methods</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          <img src="static/images/methods.png">-->
<!--        </div>-->
<!--        <br>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--          The proposed Style-Guided Diffusion Model (SGDiff) network (a) an overview and (b) detail model: SGDiff takes two inputs, a text condition (c<sub>t</sub>) for garment attributes and a style condition (c<sub>s</sub>) for style guidance, and leverages the Skip Cross-Attention (SCA) module and a pretrained CLIP image encoder for efficient training and resource utilization.-->
<!--        </h2>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!-- End youtube video -->


<!--&lt;!&ndash; Paper poster &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->

<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--&lt;!&ndash;End paper poster &ndash;&gt;-->


<!--BibTex citation-->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{,
  title={Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage},
  author={Sun, Zhengwentai and Li, Heyuan and Yang, Xihe and Zheng, Keru and Ning, Shuliang and Zhi, Yihao and Liao, Hongjie and Li, Chenghong and Cui, Shuguang and Han, Xiaoguang},
  journal={arXiv preprint arXiv:2503.},
  year={2025}
}
      </code></pre>
    </div>
  </section>
<!--End BibTex citation-->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>&copy; <span id="currentYear"></span> DiscoHuman Project</p>
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                             target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.getElementById("currentYear").innerText = new Date().getFullYear();
</script>
<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
